{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replica project\n",
    "\n",
    "### Clustering efforts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the metadata\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "\n",
    "sys.path.insert(0, \"../model/\")\n",
    "from utils import *\n",
    "\n",
    "sys.path.insert(0, \"../web_annotation/\")\n",
    "from utils_clusters import *\n",
    "from metrics_clusters import * \n",
    "\n",
    "\n",
    "data_dir = '/scratch/students/schaerf/'\n",
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_clustering = input() #'dbscan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder_dir = input() #'25-05-2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type_clustering == 'dbscan':\n",
    "    dist = float(input()) #1200\n",
    "else:\n",
    "    dist = int(input()) #1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2 = float(input()) #0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "effort = int(input()) #0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if effort == 0:\n",
    "    data_file = 'data_sample.csv'\n",
    "elif effort == 3:\n",
    "    data_file = 'data.csv'\n",
    "else:\n",
    "    data_file = subfolder_dir + '/data_retrain_'+str(effort)+'.csv'\n",
    "\n",
    "embeds_file = subfolder_dir + '/resnext-101_'+subfolder_dir+'.npy'\n",
    "map_file = subfolder_dir + '/map2pos.pkl'\n",
    "cluster_file = subfolder_dir + '/clusters_'+type_clustering+'_'+str(dist)+'_'+subfolder_dir+'_19.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07-06-2022/clusters_spectral_clustering_1500_07-06-2022_19.pkl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79536, 2)\n"
     ]
    }
   ],
   "source": [
    "clusters = make_clusters_embeddings(data_dir, dist=dist, data_file=data_file, embed_file=embeds_file, type_clustering=type_clustering, dist2=dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + cluster_file, 'wb') as outfile:\n",
    "    pickle.dump(clusters, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(data_dir + data_file).drop(columns=['level_0'])# 'data_sample.csv')\n",
    "positives = update_morph(data_dir, '-2022') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8929078483245151, 'recall': 0.7032323536837082}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_morph_scores = evaluate_morph(positives, cluster_file, set_splits = ['test', 'val'])\n",
    "cluster_morph_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting 2d position for visual clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2pos = get_2d_pos(data_dir, embed_file=embeds_file)\n",
    "with open(data_dir + map_file, 'wb') as outfile:\n",
    "    pickle.dump(map2pos, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining\n",
    "\n",
    "Annotation based scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-05-2022 {'original size': 4966, 'newly added': 224, 'additions to existing clusters': 136, 'number of clusters with new elements': 46, 'new clusters': 36, 'new clusters elements': 88, 'progress': '4.51%'}\n",
      "25-05-2022 0.41\n",
      "28-05-2022 {'original size': 4966, 'newly added': 217, 'additions to existing clusters': 163, 'number of clusters with new elements': 53, 'new clusters': 16, 'new clusters elements': 54, 'progress': '4.37%'}\n",
      "28-05-2022 0.44\n",
      "01-06-2022 {'original size': 4966, 'newly added': 212, 'additions to existing clusters': 186, 'number of clusters with new elements': 48, 'new clusters': 9, 'new clusters elements': 26, 'progress': '4.27%'}\n",
      "01-06-2022 0.51\n"
     ]
    }
   ],
   "source": [
    "for subfolder_dir in ['25-05-2022', '28-05-2022', '01-06-2022']:\n",
    "    \n",
    "    positives = update_morph(data_dir, subfolder_dir)\n",
    "    novelty_scores = novelty_score(positives, subfolder_dir)\n",
    "    print(subfolder_dir, novelty_scores)\n",
    "\n",
    "    morpho_graph_clusters = pd.read_csv(data_dir + 'morphograph_clusters.csv')\n",
    "    morpho_graph_clusters = morpho_graph_clusters[morpho_graph_clusters['cluster_file'].str.contains(subfolder_dir)]\n",
    "\n",
    "    scores_clusters = cluster_accuracy(morpho_graph_clusters)\n",
    "    print(subfolder_dir, scores_clusters)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07-06-2022 {'original size': 4966, 'newly added': 42, 'additions to existing clusters': 8, 'number of clusters with new elements': 6, 'new clusters': 8, 'new clusters elements': 34, 'progress': '0.85%'}\n",
      "07-06-2022 0.99\n"
     ]
    }
   ],
   "source": [
    "for subfolder_dir in ['07-06-2022']:\n",
    "    \n",
    "    positives = update_morph(data_dir, subfolder_dir)\n",
    "    novelty_scores = novelty_score(positives, subfolder_dir)\n",
    "    print(subfolder_dir, novelty_scores)\n",
    "\n",
    "    morpho_graph_clusters = pd.read_csv(data_dir + 'morphograph_clusters.csv')\n",
    "    morpho_graph_clusters = morpho_graph_clusters[morpho_graph_clusters['cluster_file'].str.contains(subfolder_dir)]\n",
    "\n",
    "    scores_clusters = cluster_accuracy(morpho_graph_clusters)\n",
    "    print(subfolder_dir, scores_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-05-2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13028/13028 [01:08<00:00, 189.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all positions 200.63960298654268\n",
      "min positions 63.51210898082745\n",
      "median positions 205.69374369323916\n",
      "mean average precision 0.42130403841280356\n",
      "recall @ 400 0.77763527381736\n",
      "recall @ 200 0.7180536846504237\n",
      "recall @ 100 0.6583113652859789\n",
      "recall @ 50 0.5929899953228329\n",
      "recall @ 20 0.5058607889858885\n",
      "28-05-2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13028/13028 [01:08<00:00, 190.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all positions 203.44011266598113\n",
      "min positions 62.32139253279516\n",
      "median positions 206.63925327951563\n",
      "mean average precision 0.4041070277315487\n",
      "recall @ 400 0.7653663023411815\n",
      "recall @ 200 0.7272548861374608\n",
      "recall @ 100 0.6520562339478639\n",
      "recall @ 50 0.5890477673405604\n",
      "recall @ 20 0.49320241250773944\n",
      "01-06-2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13028/13028 [01:02<00:00, 207.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all positions 205.89131309518487\n",
      "min positions 61.12865792129163\n",
      "median positions 210.72073662966702\n",
      "mean average precision 0.40791816048701385\n",
      "recall @ 400 0.7829580650169901\n",
      "recall @ 200 0.720258031442242\n",
      "recall @ 100 0.65470614163455\n",
      "recall @ 50 0.5832405151719773\n",
      "recall @ 20 0.4948517078250467\n"
     ]
    }
   ],
   "source": [
    "for subfolder_dir in ['25-05-2022', '28-05-2022', '01-06-2022']:\n",
    "    embeds_file = subfolder_dir + '/resnext-101_'+subfolder_dir+'.npy'\n",
    "    data = pd.read_csv(data_dir + data_file)\n",
    "    embeds = np.load(data_dir + embeds_file, allow_pickle=True) \n",
    "    \n",
    "    print(subfolder_dir, )\n",
    "    get_scores(embeds, data, positives, reverse_map=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-05-2022/clusters_dbscan_0.08_25-05-2022_19.pkl {'precision': 0.9512017990120183, 'recall': 0.7429868891724661}\n",
      "25-05-2022/clusters_mix_2000_25-05-2022_19.pkl {'precision': 0.8256996054683917, 'recall': 0.5669260316628028}\n",
      "28-05-2022/clusters_dbscan_0.08_28-05-2022_19.pkl {'precision': 0.9571173939604596, 'recall': 0.7451539260919626}\n",
      "28-05-2022/clusters_mix_2000_28-05-2022_19.pkl {'precision': 0.7745777991678072, 'recall': 0.5215796326504856}\n",
      "01-06-2022/clusters_dbscan_0.08_01-06-2022_19.pkl {'precision': 0.9060931130912883, 'recall': 0.7226228741647304}\n",
      "01-06-2022/clusters_mix_2000_01-06-2022_19.pkl {'precision': 0.779881047315258, 'recall': 0.5556236558471487}\n"
     ]
    }
   ],
   "source": [
    "cluster_files = ['25-05-2022/clusters_dbscan_0.08_25-05-2022_19.pkl', '25-05-2022/clusters_mix_2000_25-05-2022_19.pkl',\n",
    "                '28-05-2022/clusters_dbscan_0.08_28-05-2022_19.pkl', '28-05-2022/clusters_mix_2000_28-05-2022_19.pkl',\n",
    "                '01-06-2022/clusters_dbscan_0.08_01-06-2022_19.pkl', '01-06-2022/clusters_mix_2000_01-06-2022_19.pkl',\n",
    "                ]\n",
    "for cluster_file in cluster_files:\n",
    "    cluster_morph_scores = evaluate_morph(positives, cluster_file , set_splits = ['test', 'val'])\n",
    "    print(cluster_file, cluster_morph_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the new train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12825, 27)\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv(data_dir + data_file).drop(columns=['level_0'])# 'data_sample.csv')\n",
    "data = pd.concat([metadata, positives], axis=0).reset_index().groupby(['uid', 'uid_connection']).last().reset_index()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(data_dir + 'data_retrain_' + str(effort+1) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.drop(columns=['img1', 'img2', 'type', 'annotated', 'index', 'cluster', 'set', 'uid_connection'])\n",
    "embeddings = np.load(data_dir + embeds_file, allow_pickle=True) \n",
    "\n",
    "\n",
    "with open(data_dir + 'uid2path.pkl', 'rb') as infile:\n",
    "    uid2path = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = make_new_train_set(embeddings, positives, morpho_graph_clusters, subfolder_dir, uid2path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = sets[sets['set'] == 'train']\n",
    "val_set = sets[sets['set'] == 'val']\n",
    "\n",
    "train_set.to_csv(data_dir + 'retrain_' + str(effort+1) + '_train.csv')\n",
    "val_set.sample(frac=0.1).to_csv(data_dir + 'retrain_' + str(effort+1) + '_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder_dir = '28-05-2022'\n",
    "cluster_file = '28-05-2022/clusters_mix_2000_28-05-2022_19.pkl'\n",
    "previous_cluster_date = '25-05-2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3174, 7)\n",
      "(936, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positives': 0.72, 'negatives': 0.6892430278884463}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpho_graph_clusters = pd.read_csv(data_dir + 'morphograph_clusters.csv')\n",
    "track_cluster_progression(morpho_graph_clusters, cluster_file, previous_cluster_date, positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1720, 7)\n",
      "(840, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positives': 0.7441860465116279, 'negatives': 0.7454545454545455}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subfolder_dir = '01-06-2022'\n",
    "cluster_file = '01-06-2022/clusters_mix_2000_01-06-2022_19.pkl'\n",
    "previous_cluster_date = '28-05-2022'\n",
    "\n",
    "morpho_graph_clusters = pd.read_csv(data_dir + 'morphograph_clusters.csv')\n",
    "track_cluster_progression(morpho_graph_clusters, cluster_file, previous_cluster_date, positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3174, 7)\n",
      "(936, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positives': 0.8, 'negatives': 0.6054794520547945}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_cluster_date = '25-05-2022'\n",
    "\n",
    "morpho_graph_clusters = pd.read_csv(data_dir + 'morphograph_clusters.csv')\n",
    "track_cluster_progression(morpho_graph_clusters, cluster_file, previous_cluster_date, positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bdddb31d2fb89da9078eb0011314936146baaf4f9e5c99982bea0ddf03884f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
